---
title: "Practical 03 - Multiple linear regression and model residuals"
#author: "Michael Noonan"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width=6, fig.height=4)
```

```{r, echo = FALSE}
```
This page was last updated on `r format(Sys.time(), '%B %d, %Y')`.

* * *
## Background

In this Practical you will:

* Fit a multiple linear regression model to empirical data.
* Use simulated data to explore the difference between R-squared and adjusted R-squared
* Explore the residuals of models for data with heteroskedastic errors
* Explore the residuals of models for data with lots of 0 entries

* * *
## Multiple linear regression model
For most biological systems there are typically multiple variables that influence outcomes. To account for this, we fit multiple-linear regression models to our data. Multiple linear regression models typically tale the form:

<center>
$y_i = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... \beta_m x_m + \varepsilon_i$ 
</center>
</br>

We will explore multiple linear regression using data were collected by [Ismail et al.](https://doi.org/10.1371/journal.pone.0089437.g003) to explore the influence of forest degradation on the recruitment of Dysoxylum malabaricum. D. malabaricum is a large canopy tree species found in highly fragmented forest patches within a complex agro-forest landscape of the Western Ghats biodiversity hot spot, South India. Ismail et al. collected data on the densities of seedlings along with indicators of degradation in 17 forest patches, to investigate whether measures of degradation correlated with seedling density. The measures of degradation include i) the percentage of closed canopy; ii) the density of Clerodendrum viscosum, an early successional light demanding shrub that establishes itself well in degraded forests; and iii) the number of arboreal termite nests attached to branches and trees, which has been shown to be an effective indicator of forest disturbance, because termites are sensitive to forest disturbance and to canopy loss.


1. <ol>
* Import the Seedling dataset.
* Check for any correlations between the 3 predictor variables.
* Fit a simple linear regression model to model the relationship between Dm seedling density and Canopy closure.
* Manually calculate the the ordinary residuals, then compare them to the output of the `residuals()` function.
* Plot the ordinary residuals against the fitted values, plot a histogram of the ordinary residuals, describe what you see.
* Fit a model to the  Dm seedling density data that includes all predictor variables, what happened to the R-squared?
* Plot and inspect the ordinary residuals, describe what you see.
</ol>

```{R}
```

* * *
## R-squared and adjusted R-squared

### R-squared 

The residuals measure how accurately a fitted model predicts the observed data, and they are also used to calculate the coefficient of determination (i.e., R-squared). A model's R-squared value is the proportion of the variance in the response variable that is predictable from the predictor(s). You will often see ecologists say things like our model explained 87% of the variance in our response variable. In most cases they are getting this number from a model's R-squared.

1. <ol>
* Manually calculate the R-squared for the full model you just fit to the seedling data
* Compare this value to the output of `summary()`
</ol>

```{r}
```


The problem with R-squared values is that they don't take into account the number of parameters in the model. If we keep adding meaningless parameters to a model to soak up some of the residual variation, the R-square will necessarily decrease, which can trick us into thinking we have a better model. 

1. <ol>
* Generate a dataset of 40 normally distributed y values
* Use a for loop to sequentially add 30 predictor variables, made up of Gaussian noise, to a model predicting y.
* Extract the R-squared values from these models, and plot them against the number of parameters. Describe the effect.
</ol>

```{r}
```


### Adjusted R-squared
When you add more parameters to the model, the r-squared converges to 1. This suggests that we have an increasingly better model, but we know for a fact that most of the parameters are just noise. The reason for this is that the R-squared value doesn't factor in the number of parameters in the model. As a correction, the adjusted R-square is 'adjusted' based on the number of parameters in the model.

1. <ol>
* Generate a dataset of 40 normally distributed y values
* Again, use a for loop to sequentially add 30 predictor variables, made up of Gaussian noise, to a model predicting y.
* This time, extract the Adjusted R-squared values from these models, and plot them against the number of parameters. Describe what you see now.
</ol>

```{r}
```

Now when you add more parameters to the model, the adjusted r-squared does not converge to 1. The reason for this is that the adjusted R-squared accounts for the number of parameters in the model. When reporting R-squared values for models with multiple parameters, it's safer to report the adjusted R-squared.



***
## Heteroskedasticity

#### Problem
Sometime data will exhibit "heteroscedasticity", meaning that the residuals (i.e., variance in the data that is not explained by the model) get larger as the prediction moves from small to large values of x (or from large to small).


1. <ol>
* Describe how the following function works to generate heteroskedastic data:
```{r}
hetero <- function(x) {
  B_0 <- 0
  B_1 <- 2
  sigma = x^1.5
  eps = rnorm(x,mean=0,sd=sqrt(sigma))
  y = B_0 + B_1*x + eps
  y
}
```
* Use this function to generate 200 uniformly sampled data points between 1 and 50
* Plot the data and describe what you see
* Fit a model to these data
* Plot the residuals against the fitted values and describe the effect.
</ol>

```{r}
```

### Implications

Heteroskedasticity on its own does not inherently create a problem (notice the parameter estimates), but it's often an indication that your model can be improved. Importantly, if your sample size is small, and you can’t fix the issue, your p-values may be a bit be a bit higher or lower than they should be, so possibly a variable that is right on the border of significance may end up erroneously on the wrong side of that border. Your regression coefficients will still be generally accurate though. The accuracy of any of your predictions will also be off (either too narrow, or too wide), depending on where you fall along the x-axis.

### How to solve the issue

* The most frequently successful solution is to transform a variable, however...

* More often than not, heteroscedasticity indicates that your model is missing a component

1. <ol>
* Attempt to fix the heteroscedasticity in these data by square root transforming the y values.
*Plot the transformed data and fitted regression line
* Plot the residuals against the fitted values and describe the effect of the transformation.
</ol>
```{r}
```



```{r}
```

* * *
## Zero inflated data
It's very common in biology to have datasets with a lot of 0s. This can happen for any number of reasons, such as measurement error, detection issues, random stochasticty, and all of these combined. Such datasets contain more zeros than what one would expect to observe using the traditional model’s probability distribution, often termed zero-inflated data. This issue has a very clear effect on model residuals, and one we will explore here.

1. <ol>
* Using the seedling dataset, generate 20 data points with values of 0 for Dm_seedling_density, but evenly distributed across the range of all other predictors variables
* Combine these data with the original dataset, fit a model to the new data, and describe the fit.
* Plot the residuals against the fitted values and describe what you see.
</ol>

```{r}
```
